Upgrade the current version of kubernetes from 1.18 to 1.19.0 exactly using the kubeadm utility. Make sure that the upgrade is carried out one node at a time starting with the master node. To minimize downtime, the deployment gold-nginx should be rescheduled on an alternate node before upgrading each node.Upgrade master/controlplane node first. Drain node01 before upgrading it. Pods for gold-nginx should run on the master/controlplane node subsequently.

master node upgrade
kubectl drain controlplane --ignore-daemonsets
apt update
apt-get install kubeadm=1.19.0-00
kubeadm upgrade plan v1.19.0
kubeadm upgrade apply v1.19.0
kubectl get nodes --> this still shows the version of kubelets not apiserver...
apt-get install kubelet=1.19.0-00
systemctl restart kubelet
kubectl get nodes --> should show updated version for master node...
kubectl uncordon controlplane

worker node upgrade
kubectl drain node1 --ignore-daemonsets
apt update
apt-get install kubeadm=1.19.0-00
kubeadm upgrade node --kubelet-version v1.19.0
apt-get install kubelet=1.19.0-00
systemctl restart kubelet

Back on Master:
kubectl uncordon node1
repeat the same worker node steps to all worker nodes
---------------------------------------------------------------------------------------
Print the names of all deployments in the admin2406 namespace in the following format:
DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE
<deployment name> <container image used> <ready replica count> <Namespace>
. The data should be sorted by the increasing order of the deployment name.
Example:
DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE
deploy0 nginx:alpine 1 admin2406
Write the result to the file /opt/admin2406_data.
Hint: Make use of -o custom-columns and --sort-by to print the data in the required format.
controlplane $ kubectl get deployments -o=custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name
DEPLOYMENT   CONTAINER_IMAGE   READY_REPLICAS   NAMESPACE
deploy1      nginx             1                admin2406
deploy2      nginx:alpine      1                admin2406
deploy3      nginx:1.16        1                admin2406
deploy4      nginx:1.17        1                admin2406
deploy5      nginx:latest      1                admin2406
---------------------------------------------------------------------------------------
A kubeconfig file called admin.kubeconfig has been created in /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it.
Make sure the port for the kube-apiserver is correct.
Change port from 2379 to 6443.
Run: kubectl cluster-info --kubeconfig /root/admin.kubeconfig
---------------------------------------------------------------------------------------
Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update. Make sure that the version upgrade is recorded in the resource annotation.

kubectl create deployment nginx-deploy --image=nginx:1.16
kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record=true
---------------------------------------------------------------------------------------
A new deployment called alpha-mysql has been deployed in the alpha namespace. However, the pods are not running. Troubleshoot and fix the issue. The deployment should make use of the persistent volume alpha-pv to be mounted at /var/lib/mysql and should use the environment variable MYSQL_ALLOW_EMPTY_PASSWORD=1 to make use of an empty root password.

check the pvc status, compare with pv, delete the pvc and recreate using right storageclassname, capacity, annotations & access modes....
---------------------------------------------------------------------------------------
Take the backup of ETCD at the location /opt/etcd-backup.db on the master node
export ECTDCTL_API=3
etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 member list
etcdctl snapshot save --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 /opt/etcd-backup.db
etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 snapshot status /opt/etcd-backup.db -w table


etcdctl snapshot restore /opt/etcd-backup.db --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379  --data-dir /var/lib/etcd-backup

etcdctl snapshot restore /opt/etcd-backup.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --name=controlplane --data-dir=/var/lib/etcd-backup --initial-cluster=controlplane=https://172.17.0.13:2380 --initial-cluster-token=etcd-cluster-1 --initial-advertise-peer-urls=https://172.17.0.13:2380


After, edit /etc/kubernetes/manifests/etcd.yaml and change /var/lib/etcd to /var/lib/etcd-backup.
---------------------------------------------------------------------------------------
Create a pod called secret-1401 in the admin1401 namespace using the busybox image. The container within the pod should be called secret-admin and should sleep for 4800 seconds. The container should mount a read-only secret volume called secret-volume at the path /etc/secret-volume. The secret being mounted has already been created for you and is called dotfile-secret.

kubectl -n admin1401 run secret-1401 --image=busybox --command sleep 4800 --dry-run=client -o yaml > pod.yaml
controlplane $ cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: secret-1401
  name: secret-1401
  namespace: admin1401
spec:
  containers:
  - command:
    - sleep
	args:
    - "4800"
    image: busybox
    name: secret-admin
    volumeMounts:
    - name: secret-volume
      mountPath: /etc/secret-volume
      readOnly: true
  volumes:
    - name: secret-volume
      secret:
        secretName: dotfile-secret
---------------------------------------------------------------------------------------
kubectl get pods --show-labels
-l for label option while creating the pod
kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os_x43kj56.txt
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /pv/data-analytics
---------------------------------------------------------------------------------------
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  usages:
  - client auth
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQU1RZm1jMWpZNThPYlNYaWdNRXRQU0Zqek10S1ZaUkVzN1RWcW5kdlVyckF6alZkCm52emNWem9Lc3I4UnZUQlF6TU9DZGR6d0RNajNvYTdEdVZmSjZvYzJRQWoxbU14K1dnWHFkMkwxOG1zTUNFeHQKUjhGb2VjMEI3VWMyWEJhVVpXNm5GNnVPT0NmYmw1TzhvUkw3ZE5jMHdSazdjMW5VejMvU051MUZyZ0dqanJoTApJckRmU3JodHhrMHJ3dldUSkpkWGlEOEQ2bnh0WFlWQjlqWFNETlJRbG10ZWtIU2ttOFRGVXZWZjBFOURsY3B3CnUvbFhTdVMyU01JTW02bHFVUHNUMFlhcmRQM3dIR1Q3K0w2VWliNE1vQ1F6S3ZNVGIxRTJaTmxBMjBOSjJ5aUYKY0d5by9KN200NGJMZWU0bU1oaUU5bEVVY3M5VnJJUmVmTGhIdFdVQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQ1lJWVhjVGRtWEUwcTYxaUVEMVl5REd4bi81OWhORWMrcUdNbysvb09WQ0FRWVBsYTFjRTNYCkxYQ3poSml0a2VjRi95RFRNOGpCZmZLZmRLcDhYMzA4RTNYZEwxZ2NmNHVDTVVnUHp6bHA2M2c4TlA4L2hLWDIKQjJmV29YYjRndVJTZThTYlZNSHVkcTVOaXBrTEZHTXRpU3M3N1BxME1Eb2NqcFhoYWp6M0I1eGFqczNablppVAp4YStKTDRqWEd5dmpnZ1psL3NnVlMza2cxT2diQUYyOS80ckhhR3pJb2Z1c3d5ZVRxR0d6UkhXcjhYUjNzbnJXCkRGMmp4c2xHN255RVl1WUlDV3NvcDlzUExoWTRkRWZ6QWdoK0NEeHd1NHZBbVV3NnNEVEhXb05iMkUyL0I3RnQKaEFORmhlRFJ0aHJDZkg0VVo3eXZtWi9xTnNyL1c4enkKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
  signerName: kubernetes.io/kube-apiserver-client
  groups:
  - system:authenticated
  
kubectl get csr
kubectl certificate approve john-developer
kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development
kubectl create rolebinding developer-role-binding --role=developer --user=john --namespace=development
kubectl -n development describe role
kubectl -n development describe rolebinding
kubectl -n development auth can-i update/list/delete/watch pods --as=john

kubectl run busybox1 --generator=run-pod/v1 --image=busybox:1.28 -- sleep 3600
kubectl exec -ti busybox1 -- nslookup nginx-resolver-service > /root/CKA/nginx.svc
kubectl exec -it busybox1 -- nslookup 10-244-1-8.default.pod > /root/CKA/nginx.pod

Create a Pod called redis-storage with image: redis:alpine with a Volume of type emptyDir that lasts for the life of the Pod. Specs on the right.
controlplane $ cat redis.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: redis-storage
  name: redis-storage
spec:
  containers:
  - image: redis:alpine
    name: redis-storage
    volumeMounts:
    - mountPath: /data/redis
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
	
Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time.The container should sleep for 4800 seconds
controlplane $ cat super-user-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: super-user-pod
  name: super-user-pod
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: busybox:1.28
    name: super-user-pod
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
		
A pod definition file is created at /root/CKA/use-pv.yaml. Make use of this manifest file and mount the persistent volume called pv-1. Ensure the pod is running and the PV is bound. mountPath: /data persistentVolumeClaim Name: my-pvc
controlplane $ cat pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
controlplane $ cat /root/CKA/use-pv.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image: nginx
    name: use-pv
    volumeMounts:
    - mountPath: /data
      name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: my-pvc

Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Record the version. Next upgrade the deployment to version 1.17 using rolling update. Make sure that the version upgrade is recorded in the resource annotation.
kubectl run  nginx-deploy --image=nginx:1.16 --replicas=1 --record
=================================================================================================================================
kubectl create serviceaccount pvviewer
kubectl create clusterrole pvviewer-role --resource=persistentvolumes --verb=list
kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pvviewer
  name: pvviewer
spec:
  containers:
  - image: redis
    name: pvviewer
  serviceAccountName: pvviewer

List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.Answer should be in the format: InternalIP of master<space>InternalIP of node1<space>InternalIP of node2<space>InternalIP of node3 (in a single line)
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'

Create a pod called multi-pod with two containers. Container 1, name: alpha, image: nginx Container 2: beta, image: busybox, command sleep 4800. Environment Variables: container 1: name: alpha Container 2: name: beta
controlplane $ cat multi.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: multi-pod
  name: multi-pod
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: busybox
    name: beta
    env:
    - name: name
      value: beta
  - command:
    image: nginx
    name: alpha
    env:
    - name: name
      value: alpha

Create a Pod called non-root-pod , image: redis:alpine
runAsUser: 1000
fsGroup: 2000
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - image: redis:alpine
    name: non-root-pod
	

We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it.Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80. Important: Don't delete any current objects deployed.
controlplane $ kubectl run test --image=busybox:1.28 --rm -it -- sh
If you don't see a command prompt, try pressing enter.
/ # nc -zvw 4 10.32.0.3:80
nc: 10.32.0.3:80 (10.32.0.3:80): Connection timed out

kubectl run test-np --image=busy-box:1.28 --rm -it -- sh
kubectl get netpol
kubectl describe netpol 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
  - Ingress
  ingress:
    - ports:
        - protocol: TCP
          port: 80
controlplane $ kubectl run test --image=busybox:1.28 --rm -it -- sh
If you don't see a command prompt, try pressing enter.
/ # nc -zvw 4 10.32.0.3:80
10.32.0.3:80 (10.32.0.3:80) open
/ #	  

Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine to ensure workloads are not scheduled to this worker node. Finally, create a new pod called prod-redis and image redis:alpine with toleration to be scheduled on node01.key:env_type, value:production, operator: Equal and effect:NoSchedule
kubectl taint node node01 env_type=production:NoSchedule
kubectl describe node node01 | grep -i taint
controlplane $ cat tol.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: prod-redis
  name: prod-redis
spec:
  containers:
  - image: redis:alpine
    name: prod-redis
  tolerations:
  - key: env_type
    operator: Equal
    effect: NoSchedule
	
Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier .
image: redis:alpine
Use appropriate labels and create all the required objects if it does not exist in the system already.
controlplane $ cat hr.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    environment: production
    tier: frontend
  name: hr-pod
  namespace: hr
spec:
  containers:
  - image: redis:alpine
    name: hr-pod
	
We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.
=============================================================================================================================
kubectl config set-context --current --namespace=kube-system
kubectl get pods,svc,deployments

export ETCDCTL_API=3

kubectl -n kube-system describe pod etcd-controlplane

ETCDCTL_API=3 etcdctl member list --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379

ETCDCTL_API=3 etcdctl snapshot save --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 /opt/snapshot-pre-boot.db

etcdctl --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db

ETCDCTL_API=3 etcdctl snapshot restore --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 --data-dir="/var/lib/etcd-from-backup" --initial-cluster="controlplane=https://127.0.0.1:2380" --name="controlplane" --initial-advertise-peer-urls="https://127.0.0.1:2380" --initial-cluster-token="etcd-cluster-1" /opt/snapshot-pre-boot.db

cd /etc/kubernetes/manifests
vi etcd.yaml

--data-dir=/var/lib/etcd-from-backup
--initial-cluster-token=etcd-cluster-1
mountPath=/var/lib/etcd-from-backup  (volume mounts and hostpath)

watch "docker ps -a | grep etcd"
kubectl get pods,svc,deployments
================================================================================================================================
kubectl get nodes -o jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.addresses[?(@.type=="InternalIP")].address}{"\t"}'
kubectl get nodes -o custom-columns=Nodename:.metadata.name,Taint:.spec.taints[].effect
kubectl top pod --sort-by=cpu --no-headers | head -1 | awk -F ' ' '{print $1}'
nodes in ready state other than tainted nodes count redirect
kubectl get nodes -o jsonpath="{range .items[*]}{.metadata.name} {.spec.taints[?(@.effect=='NoSchedule')].effect}{\"\n\"}{end}" | grep -v NoSchedule |wc -l

kubectl get nodes -o jsonpath='{.items[*].metadata.name}  {.items[*].status.addresses[?(@.type=="InternalIP")].address}'
ingress
curl -kL internalip/hello should result hello
5678
/hello
ing-namespace
root@kubemaster:~# cat hello.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: hello
  name: hello
spec:
  containers:
  - image: hashicorp/http-echo
    name: hello
    args:
      - "-text=hello"
kubectl expose pod hello --name=hello-service --port=5678
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello-service
            port:
              number: 5678

persistentvolumecliam expansion and record the change, storage class
kubectl edit or kubectl patch
kubectl patch pvc my-pvc -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}' -- record
persistentvolumeclaim/mongodb-pv-claim patched

network policy
allow traffic from internal namespace pod's to 9200 port in echo name space pod's 
don't allow access to pod's which don't listen on 9200 port
don't allow access from pod's which are not part of internal name space
kubectl edit namespace internal and add labels.
apiVersion: v1
kind: Namespace
metadata:
  labels:
    name: internal
  name: internal
  
or

kubectl label namespace/dev purpose=testing
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: echo
spec:
  podSelector: {}
  policyTypes:
  - Ingress

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: echo
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: internal
    ports:
    - protocol: TCP
      port: 6379
	   
sidecar for log monitoring along volumemount. env: log_filename, /var/log/11-factor-app.log
	
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        i=$((i+1));
        sleep 1;
      done      
    volumeMounts:
    - name: varlog
      mountPath: /var/log
	- name: logifle
	  mountPath: /var/log/11-app-factor.log
  - name: sidecar
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/11-app-factor.log']
    volumeMounts:
    - name: logfile
      mountPath: /var/log/11-app-factor.log
  volumes:
  - name: logfile
    hostPath:
      path: /var/log/11-app-factor.log
		  
		  
journalctl -u kubelet | grep Errors